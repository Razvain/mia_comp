{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed3d01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz import process\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3645b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results found: 10\n",
      "Sleeping for 3 seconds\n",
      "Sleeping for 3 seconds\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import time\n",
    "import feedparser\n",
    "\n",
    "base_url = 'http://export.arxiv.org/api/query?'\n",
    "\n",
    "search_query = (\n",
    "    'all:(\"llm\" OR \"language model\") AND (\"se\" OR \"software engineer\" OR '\n",
    "    '\"software develop\" OR \"code\" OR \"coding\" OR \"program\" OR \"requirement\" OR \"bug\" OR \"test\" OR '\n",
    "    '\"debug\" OR \"refactor\" OR \"vulnerab\" OR \"fault\" OR \"clon\")'\n",
    ")\n",
    "\n",
    "all_links = []\n",
    "start = 0\n",
    "results_per_iteration = 5\n",
    "wait_time = 3\n",
    "\n",
    "params = {\n",
    "    'search_query': search_query,\n",
    "    'start': start,\n",
    "    'max_results': results_per_iteration\n",
    "}\n",
    "query = urllib.parse.urlencode(params)\n",
    "url = base_url + query\n",
    "response = urllib.request.urlopen(url).read()\n",
    "feed = feedparser.parse(response)\n",
    "\n",
    "# total_results = int(feed.feed.opensearch_totalresults)\n",
    "total_results = 10  # Placeholder for total results, adjust as needed\n",
    "print(\"Total results found:\", total_results)\n",
    "\n",
    "for i in range(0, total_results, results_per_iteration):\n",
    "    params['start'] = i\n",
    "    query = urllib.parse.urlencode(params)\n",
    "    url = base_url + query\n",
    "\n",
    "    response = urllib.request.urlopen(url).read()\n",
    "    feed = feedparser.parse(response)\n",
    "\n",
    "    for entry in feed.entries:   \n",
    "        all_links.append(entry.links[1].href)\n",
    "\n",
    "    print(f'Sleeping for {wait_time} seconds')\n",
    "    time.sleep(wait_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2590501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emails found so far: {'mika.saari@tuni.fi', 'shahbaz.siddeeq@tuni.fi', 'muhammad.waseem@tuni.fi', 'henri.terho@eficode.com', 'mdmahade.hasan@tuni.fi', 'Kai-Kristian.Kemell@tuni.fi', 'zeeshan.rasheed@tuni.fi', 'kalle.makela@gmail.com', 'pekka.abrahamsson@tuni.fi', 'jussi.rasku@tuni.fi'}\n",
      "All emails found so far: {'mika.saari@tuni.fi', 'shahbaz.siddeeq@tuni.fi', 'muhammad.waseem@tuni.fi', 'henri.terho@eficode.com', 'mdmahade.hasan@tuni.fi', 'Kai-Kristian.Kemell@tuni.fi', 'zeeshan.rasheed@tuni.fi', 'kalle.makela@gmail.com', 'rubenm@cs.cmu.edu', 'vhellendoorn@cmu.edu', 'pekka.abrahamsson@tuni.fi', 'clegoues@cs.cmu.edu', 'aidan@cmu.edu', 'jussi.rasku@tuni.fi'}\n",
      "All emails found so far: {'mika.saari@tuni.fi', 'shahbaz.siddeeq@tuni.fi', 'muhammad.waseem@tuni.fi', 'henri.terho@eficode.com', 'btaghiz@ncsu.edu', 'mdmahade.hasan@tuni.fi', 'Kai-Kristian.Kemell@tuni.fi', 'zeeshan.rasheed@tuni.fi', 'kalle.makela@gmail.com', 'rubenm@cs.cmu.edu', 'vhellendoorn@cmu.edu', 'pekka.abrahamsson@tuni.fi', 'clegoues@cs.cmu.edu', 'aidan@cmu.edu', 'hreiche@ncsu.edu', 'jussi.rasku@tuni.fi', 'permissions@acm.org'}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import fitz  \n",
    "import re\n",
    "\n",
    "def extract_emails_from_pdf_url(pdf_url):\n",
    "    try:\n",
    "        # Download PDF into memory\n",
    "        response = urllib.request.urlopen(pdf_url)\n",
    "        pdf_data = response.read()\n",
    "        \n",
    "        # Open PDF from bytes\n",
    "        pdf_document = fitz.open(stream=pdf_data, filetype=\"pdf\")\n",
    "        \n",
    "        full_text = \"\"\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            full_text += page.get_text()\n",
    "        \n",
    "        # Regex to extract normal emails\n",
    "        emails = set(re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', full_text))\n",
    "        \n",
    "        # Regex to extract alias groups like {user1,user2}@domain.com\n",
    "        alias_pattern = re.compile(r'\\{([^\\}]+)\\}@([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})')\n",
    "        for match in alias_pattern.finditer(full_text):\n",
    "            users = match.group(1)\n",
    "            domain = match.group(2)\n",
    "            usernames = [u.strip() for u in users.split(',')]\n",
    "            for u in usernames:\n",
    "                emails.add(f\"{u}@{domain}\")\n",
    "        \n",
    "        return emails\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract from {pdf_url}: {e}\")\n",
    "        return set()\n",
    "    \n",
    "all_emails = set()\n",
    "batch_size = 10\n",
    "waiting_time = 3  \n",
    "for i in range(0, len(all_links), batch_size):\n",
    "    batch = all_links[i:i + batch_size]\n",
    "    for pdf_url in batch:\n",
    "        emails = extract_emails_from_pdf_url(pdf_url)\n",
    "        all_emails.update(emails)\n",
    "    time.sleep(waiting_time) \n",
    "    # print(\"All emails found so far:\", all_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a975bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"emails.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emails, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "\n",
    "def send_email(sender_email, sender_password, recipient_email, subject, body):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg['Subject'] = subject\n",
    "\n",
    "    msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587) \n",
    "        server.starttls() \n",
    "        server.login(sender_email, sender_password)\n",
    "        server.sendmail(sender_email, recipient_email, msg.as_string())\n",
    "        server.quit()\n",
    "        print(f\"Email sent to {recipient_email}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n",
    "\n",
    "sender = \"your_email@gmail.com\"\n",
    "password = \"your_password_or_app_password\"\n",
    "recipient = \"recipient@example.com\"\n",
    "subject = \"Test Email\"\n",
    "body = \"Hello! This is an automatic email.\"\n",
    "\n",
    "send_email(sender, password, recipient, subject, body)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
